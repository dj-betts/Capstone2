{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline \n",
    "\n",
    "![](images/pipeline-walkthrough1.png)\n",
    "\n",
    "Below is a to do list when converting text into vector form: \n",
    "\n",
    "**Clean text and Create a Bag of Words (BoW)**\n",
    ">1. Lowercase the text\n",
    "2. Tokenize \n",
    "3. Strip out punctuation or undesirable text\n",
    "4. Remove Stopwords \n",
    "5. Stemming or Lemmatizing\n",
    "6. Compute N-Grams\n",
    "7. Use this to create BoW\n",
    "\n",
    "**Vectorize BoW**\n",
    ">8. Term Frequencies\n",
    "9. Document Frequencies\n",
    "10. TF-IDF\n",
    "11. Normalize vectors\n",
    "\n",
    "Let's go through both what each of these steps are and how to do them in python with the following corpus of comments about data science...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to cut off extra intro paragraphs from beautiful soup scrape\n",
    "\n",
    "# def trim_fat(string):\n",
    "#     return string[35:-115]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to vectorize the type_of_material series into a y target vector.\n",
    "def vectorize_type(ser):\n",
    "    y = ser.copy()\n",
    "    y.replace({'Op-Ed': 1,'News': 0}, inplace=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate of correct predictions out of total predictions\n",
    "def metrics_(tn, fp, fn, tp):\n",
    "    accuracy = (tp + tn) / (tn + fn + tp + fp)\n",
    "    print(f'accuracy = {accuracy}')\n",
    "    recall = (tp) / (tp + fn)\n",
    "    print(f'recall = {recall}')\n",
    "    precision = (tp) / (tp + fp)\n",
    "    print(f'precision = {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 214 ms, total: 1.56 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_2019 = pd.read_csv('data/drop2019_text_type.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 761 µs, sys: 16 µs, total: 777 µs\n",
      "Wall time: 777 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_2019_df = _2019.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X. X is currently pandas series of unsplit strings\n",
    "\n",
    "X = _2019_df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manual punctuation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lil_punc = '!\"#$%&\\()*+,-./:;<=>@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 367 ms, total: 24.1 s\n",
      "Wall time: 24.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        ' 'In his essay “The CrackUp” F Scott Fitzgera...\n",
       "1        ' 'In 2019 here’s what we could do instead' In...\n",
       "2        ' 'Many years before back in Russia the two yo...\n",
       "3        ' 'At a critical moment in the film just after...\n",
       "4        ' 'No other country in the world symbolizes th...\n",
       "                               ...                        \n",
       "37723    ' 'The two men have a history of friction and ...\n",
       "37724    ' 'Inside his checked luggage wrapped in a pla...\n",
       "37725    ' 'Do you remember dear reader when it was imp...\n",
       "37726    ' 'Chief Gallagher was acquitted this summer o...\n",
       "37727    ' 'Draft letters posted online Tuesday by the ...\n",
       "Name: text, Length: 37728, dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X.apply(punc_strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that removes string.punctuation w/out the '?'\n",
    "def punc_strip(string):\n",
    "    my_lil_punc_string = '!\"#$%&\\()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "\n",
    "    for char in string:\n",
    "        if char in my_lil_punc_string:  \n",
    "            string = string.replace(char, \"\")\n",
    "            \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y as a series of op-ed or news\n",
    "\n",
    "y = _2019_df.type_of_material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 1.79 ms, total: 12.2 ms\n",
      "Wall time: 11.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vectorize y in to (1, 0) (op-ed, news)\n",
    "\n",
    "y = vectorize_type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn series into list...\n",
    "\n",
    "corpus = list(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn TfidfVectorizer(stop_words='english', strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #create vectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(#input='content', \n",
    "# #                 encoding='utf-8', \n",
    "# #                 decode_error='strict', \n",
    "#                  strip_accents=None, \n",
    "#                  lowercase=True, \n",
    "# #                 preprocessor=None, \n",
    "# #                 tokenizer=None, \n",
    "# #                 analyzer='word', \n",
    "#                  stop_words='english', \n",
    "# #                 token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "# #                 ngram_range=(1, 1), \n",
    "# #                 max_df=1.0, \n",
    "# #                 min_df=1, \n",
    "#                  max_features=None, \n",
    "# #                 vocabulary=None, \n",
    "# #                 binary=False, \n",
    "# #                 dtype=<class 'numpy.float64'>, \n",
    "# #                 norm='l2', \n",
    "# #                 use_idf=True, \n",
    "# #                 smooth_idf=True, \n",
    "# #                 sublinear_tf=False\n",
    "# )\n",
    "# X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# stop_words = vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resample class size w/ imbalanced learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #balance the classes\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #X, y --> X_resampled, y_resampled\n",
    "# X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "\n",
    "# #return a list of tuples for item, and count of item. in this case 4139 each\n",
    "# print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #test, train, split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes/imbalanced learn/TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# multinm_clf = MultinomialNB()\n",
    "# multinm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinm_clf.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = multinm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #clf.score = accuracy = 'true'(pos/neg) / total\n",
    "\n",
    "# multinm_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion_matrix(y_true = y_test, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = y_pred).ravel()\n",
    "# (tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rate of correct predictions out of total predictions\n",
    "\n",
    "# accuracy = (tp + tn) / (tn + fn + tp + fp)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # rate of actual op-ed articles, out of all the actual od-ed articles\n",
    "\n",
    "# recall = (tp) / (tp + fn)\n",
    "# recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rate of correct predictions of op-ed articles out of all predictions\n",
    "\n",
    "# precision = (tp) / (tp + fp)\n",
    "# precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rate of correct predictions out of total predictions\n",
    "\n",
    "# accuracy = (tp + tn) / (tn + fn + tp + fp)\n",
    "# print(f'accuracy = {accuracy}')\n",
    "# recall = (tp) / (tp + fn)\n",
    "# print(f'recall = {recall}')\n",
    "# precision = (tp) / (tp + fp)\n",
    "# print(f'precision = {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sklearn.ensemble.RandomForestClassifier(n_estimators=100, \n",
    "#                                               *, \n",
    "#                                               criterion='gini', \n",
    "#                                               max_depth=None, \n",
    "#                                               min_samples_split=2, \n",
    "#                                               min_samples_leaf=1, \n",
    "#                                               min_weight_fraction_leaf=0.0, \n",
    "#                                               max_features='auto', \n",
    "#                                               max_leaf_nodes=None, \n",
    "#                                               min_impurity_decrease=0.0, \n",
    "#                                               min_impurity_split=None, \n",
    "#                                               bootstrap=True, \n",
    "#                                               oob_score=False, \n",
    "#                                               n_jobs=None, \n",
    "#                                               random_state=None, \n",
    "#                                               verbose=0, \n",
    "#                                               warm_start=False, \n",
    "#                                               class_weight=None, \n",
    "#                                               ccp_alpha=0.0, \n",
    "#                                               max_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# rf_clf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #clf.score = accuracy = 'true'(pos/neg) / total\n",
    "\n",
    "# rf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(y_true = y_test, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = y_pred).ravel()\n",
    "# (tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rate of correct predictions out of total predictions\n",
    "\n",
    "# accuracy = (tp + tn) / (tn + fn + tp + fp)\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rate of actual op-ed articles, out of all the actual od-ed articles\n",
    "\n",
    "# recall = (tp) / (tp + fn)\n",
    "# recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rate of correct predictions of op-ed articles out of all predictions\n",
    "\n",
    "# precision = (tp) / (tp + fp)\n",
    "# precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rate of correct predictions out of total predictions\n",
    "\n",
    "# accuracy = (tp + tn) / (tn + fn + tp + fp)\n",
    "# print(f'accuracy = {accuracy}')\n",
    "# recall = (tp) / (tp + fn)\n",
    "# print(f'recall = {recall}')\n",
    "# precision = (tp) / (tp + fp)\n",
    "# print(f'precision = {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sklearn.ensemble.RandomForestClassifier(n_estimators=100, \n",
    "#                                               *, \n",
    "#                                               criterion='gini', \n",
    "#                                               max_depth=None, \n",
    "#                                               min_samples_split=2, \n",
    "#                                               min_samples_leaf=1, \n",
    "#                                               min_weight_fraction_leaf=0.0, \n",
    "#                                               max_features='auto', \n",
    "#                                               max_leaf_nodes=None, \n",
    "#                                               min_impurity_decrease=0.0, \n",
    "#                                               min_impurity_split=None, \n",
    "#                                               bootstrap=True, \n",
    "#                                               oob_score=False, \n",
    "#                                               n_jobs=None, \n",
    "#                                               random_state=None, \n",
    "#                                               verbose=0, \n",
    "#                                               warm_start=False, \n",
    "#                                               class_weight=None, \n",
    "#                                               ccp_alpha=0.0, \n",
    "#                                               max_samples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_2019_df = _2019.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X. X is currently pandas series of unsplit strings\n",
    "\n",
    "X_nltk = nltk_2019_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y as a series of op-ed or news\n",
    "\n",
    "y_nktk = nltk_2019_df.type_of_material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.58 ms, sys: 970 µs, total: 10.6 ms\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vectorize y in to (1, 0) (op-ed, news)\n",
    "\n",
    "y_nltk = vectorize_type(y_nktk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn series into list...\n",
    "\n",
    "corpus_nltk = list(X_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize w/ nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# regex_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# tokenized_punc = [regex_tokenizer.tokenize(article.lower())for article in corpus_nltk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (len(tokenized_punc[0])) #2218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokenized_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# tokenized = [word_tokenize(article.lower()) for article in corpus_nltk]\n",
    "\n",
    "# CPU times: user 4min 46s, sys: 2.39 s, total: 4min 48s\n",
    "# Wall time: 4min 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list of list of strings. one list of strings per documents. list are various lengths around 1000\n",
    "\n",
    "# len(tokenized[0]) #2596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take out stop work via ntlk. does this work against sklearn when i vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# stop = set(stopwords.words('english'))\n",
    "# tokenized_docs = [[word for word in words if word not in stop]\n",
    "#             for words in tokenized_punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hopefully this reduced the number of strings / list\n",
    "\n",
    "# len(tokenized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #docs is new tokenized, but with stop words removed\n",
    "\n",
    "# len(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "# wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# docs_porter = [[porter.stem(word) for word in words]\n",
    "#                    for words in docs]\n",
    "# docs_snowball = [[snowball.stem(word) for word in words]\n",
    "#                      for words in docs]\n",
    "# docs_wordnet = [[wordnet.lemmatize(word) for word in words]\n",
    "#                     for words in docs]\n",
    "\n",
    "# CPU times: user 14min 59s, sys: 18.4 s, total: 15min 18s\n",
    "# Wall time: 15min 26s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# docs_porter = [[porter.stem(word) for word in words]\n",
    "#                    for words in docs]\n",
    "\n",
    "# CPU times: user 7min 16s, sys: 5.21 s, total: 7min 21s\n",
    "# Wall time: 7min 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# snowball_stemm = [[snowball.stem(word) for word in words]\n",
    "#                      for words in tokenized_docs]\n",
    "\n",
    "# # CPU times: user 5min 5s, sys: 5.98 s, total: 5min 11s\n",
    "# # Wall time: 5min 13s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# docs_wordnet = [[wordnet.lemmatize(word) for word in words]\n",
    "#                     for words in docs]\n",
    "\n",
    "# CPU times: user 1min 24s, sys: 4.5 s, total: 1min 28s\n",
    "# Wall time: 1min 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ## Print the stemmed and lemmatized words from the first document\n",
    "# print(\"%16s %16s %16s %16s\" % (\"word\", \"porter\", \"snowball\", \"lemmatizer\"))\n",
    "# for i in range(min(len(docs_porter[0]), len(docs_snowball[0]), len(docs_wordnet[0]))):\n",
    "#     p, s, w = docs_porter[0][i], docs_snowball[0][i], docs_wordnet[0][i]\n",
    "#     if len(set((p, s, w))) != 1:\n",
    "#         print(\"%16s %16s %16s %16s\" % (docs[0][i], p, s, w))\n",
    "#         print(docs[0][i], w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs and lemmatizer are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I choose SNOWBALL!!!! to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"', 'In his essay “The Crack-Up,” F. Scott Fitzgerald wrote, “The test of a first-rate intelligence is the ability to hold two opposed ideas in the mind at the same time, and still retain the ability to function.”', 'On New Year’s Day 1919, the headlines in The New York Times gave a hint of how difficult that would be for Americans, struggling to live up to the shimmering promises they had made to the world during the Great War, which had ended just over a month before.', 'Poles, newly independent but already threatened by their neighbors, were calling on Americans to protect them. Other countries, like Ireland, were on the cusp of independence, but asking for help. A bewildering set of new responsibilities was settling upon a country that had long cherished its freedom from the Old World.', 'Overwhelmingly, Americans longed to get their old lives back. But there was no clear road map to “normalcy,” to use a word that began to gain traction in the chaos of the postwar years. Grammarians faulted it as a clumsy neologism, but that only made it more popular. A rakish Ohio senator, Warren Harding, would eventually ride it all the way to the White House. In a crackly old recording, he can be heard promising, vaguely, to make America great again. It didn’t work out well.', 'There were reasons to believe lofty promises as 1919 was beginning. With peace, everything felt new again. That was certainly true in the geopolitical realm, as negotiators convened in Paris, eager to redraw the world map. It was difficult to know where to begin. The German, Austrian and Ottoman empires were now part of the ash heap of history, in the memorable phrase of Leon Trotsky. His country, Russia, had not disappeared, but it was undergoing a transformation so profound that it was destabilizing half of Europe. Overseas, territories belonging to the belligerents were being exchanged like carpets at a second-hand bazaar. A century later, from the Middle East to Africa, the world is still struggling with the decisions made hastily in 1919.', 'But if anyone could solve these nasty problems, Americans could. To a beleaguered world, they seemed almost superhuman. Despite their late entry into the war, they had shaped the conflict decisively with their manpower, their matériel and their chirpy marketing. “Democracy” was the word of the hour, an elixir for all problems, according to President Woodrow Wilson, who intoned it with impressive solemnity.', 'For a time, the world believed it. As he headed to Paris, for what was the longest overseas trip of a president, he was lionized as few leaders have ever been. Film footage captures the odd contrast of his quiet oceanic passage, followed by the euphoria of the crowds in France, who hoped he would solve all of their problems. That was a tall order.', 'We still live close to that grainy footage, inside the world that emerged from Versailles and its Hall of Mirrors. German resentment of the harsh conditions imposed fueled the rise of Adolf Hitler, who, like many, found his political voice in 1919. Historians would later trace the origins of World War II to the treaty’s shortcomings, and the swirling nationalisms that it failed to calm. President Wilson promised to build a world “safe for democracy,” but the result was neither safe nor, in many cases, democratic. A British politician whose career barely survived the war, Winston Churchill, saw it more clearly as a “crippled, broken world,” divided along fault lines that had never been obliterated.', 'Large parts of the British, French and Belgian empires remained intact, to the distress of millions hoping for democracies of their own. Inspired by the winds of change, independence movements sprang up around the world. Some were successful, but for many others, self-determination, another one of Wilson’s ringing phrases, was elusive at best. Weary European diplomats chafed at his moralistic speeches, which they scorned as “sermonettes.” The president of France, Georges Clemenceau, complained that “talking to Wilson was something like conversing with Jesus Christ!” Wilson would surely have seconded the thought.', 'Still, there were pockets of realism within the idealism of a politician who, for all his blind spots, understood that the world could not continue as before. Wilson has long been criticized for his failures, but he was correct in his grasp of a core truth, still relevant, that a lazy return to unbridled nationalism was a recipe for conflict. Like most presidents, he did not relish consulting with Congress, a misreading of democracy that boomeranged on him, as a newly elected Republican majority refused to approve his plan for a League of Nations.', 'Desperate to outflank them, he tried to take his case directly to the people, with boisterous rallies, far from East Coast elites — another way in which 2019 mirrors 1919. The strain of that effort resulted in a stroke, which nearly incapacitated Wilson for the last year of his presidency. Democracy’s great spokesman had imperiled the very system of government that he went to such lengths to defend. His crack-up was one of many in 1919.', 'To young writers like Fitzgerald, who received his first book contract in 1919, it was obvious that the old words were not working and new ones needed to be found. Some writers simply invented their own — James Joyce was constantly writing new ones in his scribbledehobble, or notebook.', 'Others lamented what had vanished, while dreading what was to come next. “Many ingenious lovely things are gone,” William Butler Yeats wrote, in a poem titled “Nineteen hundred and Nineteen.” Less than five years had elapsed since the war started in 1914, but everything felt different. To Yeats, the new sights and sounds were terrifying: In one of his most famous poems, “The Second Coming,” he imagined an apocalypse populated by fantastic beasts that would give J.K. Rowling a run for her money. Despite its gloom, it has become one of the most over-quoted poems in history.', 'But for millions of others, the sights and sounds of 1919 were glorious. The waging of the war had released a galvanic energy from Americans, who attacked the problem of victory with stunning efficiency. That included tremendous contributions from immigrants, women and African-Americans, who were raucously celebrated in victory parades upon return. One parade in New York feted the famous Harlem Hellfighters regiment, marching behind a spirited band playing a kind of music most Americans had never heard, soon to be known as jazz.', 'One of the hottest songs of 1919 was a novelty hit, “How Ya Gonna Keep ’Em Down on the Farm (After They’ve Seen Paree)?” It was a fair question for a country that was not going to return easily to prewar attitudes about race and citizenship. The return of the doughboys brought an exuberant close to the war effort, but also exacerbated tensions over who was entitled to democracy and self-determination, those pesky phrases that Woodrow Wilson kept repeating.', 'Throughout 1919, that was another way in which Americans seemed to hold two opposing thoughts at the same time. Large numbers of African-Americans chose their own form of self-determination by moving out of the South, toward Northern cities, the Midwest and the West Coast, a demographic change that altered America forever. But even after the move, many found democracy a daily challenge.', 'That year in the South, two children born near each other, on either side of the Alabama-Georgia line, revealed how quickly the country was changing. Jackie Robinson grew up in a new kind of America, thanks to his mother, who moved with him to California. George Wallace spent most of life trying to hold back the tide of change that Robinson’s generation helped unleash.', 'Throughout a long year, these tensions simmered just below America’s gleaming surfaces. An ugly race riot in Chicago took the lives of 38 people in late July. Other anxieties targeted immigrants, who found Americans less welcoming than the statue that greeted them in New York Harbor. To defenders of the old order, America had done her part, and there was no need to open the floodgates. But to many others, seeking a broader form of democracy, Wilson’s rhetoric seemed to describe a country that existed more vividly in his imagination than reality.', 'Torn in these two directions, the world’s wealthiest country seemed headed to its own form of a crack-up. Throughout 1919, waves of fear paralyzed Americans, as they reeled from a deadly influenza epidemic at the start of the year, then a campaign of letter bombs sent by anarchists in the spring. A vigorous government response led to the creation of the F.B.I. and a series of raids conducted by an overzealous attorney general, A. Mitchell Palmer. These “Palmer Raids” awakened fears of a police state and once again revealed how difficult democracy could be. A retaliatory bomb, directed at Palmer, exploded outside his house in Washington, nearly killing his neighbor, Franklin Roosevelt. When Roosevelt gave the speech that named the New Deal, 13 years later, he remembered Wilson who, for all of his failings, had ushered Americans into a new era.', 'A long century has elapsed, but the contradictions of 1919 still bear scrutiny. The old fault lines seem closer to the surface than they did in Roosevelt’s day, when Americans were largely united behind the great efforts to end poverty and crush fascism.', 'A hundred years ago, it was possible to look at the same country and see two very different versions of America. To be sure, this was a thriving nation of immigrants, innovating at a dizzying rate, with bustling cities that were the envy of the world. It was also a quieter country, dominated by its farmers and small towns, politically and socially conservative, and dismayed by the excesses of what would soon be known as the Jazz Age. Prohibition was another achievement — for what it’s worth — of 1919.', 'Neither side had a monopoly on virtue, as events would prove. Urban and rural America produced an equal share of political rascals. And rascality, in various guises, was another feature of life, as new forms of entertainment began to dominate attention, demanding villains and heroes in equal measure. In many cases, the war had accelerated this trend, advancing research in radio communication and using newsreels to mobilize support for the military effort. Soon, mass communication was affecting all Americans, rural and urban alike. Huge new movie palaces were built — the Capitol, in New York, could seat 4,000 — and Hollywood continued to grow apace, a new kind of dream factory, built around studios and stars.', 'Sports was a big business as well — in one case, a bit too big, as mobsters were able to alter the outcome of the 1919 World Series. Money affected fans in other ways, too — when the owner of the Boston Red Sox traded Babe Ruth to the New York Yankees at the end of the year, he invoked a curse on his team, which would not win another World Series until 2004.', 'The rural-urban divide was moderated in another way. Sales of automobiles nearly doubled in 1919, as Americans happily returned to their leisure pursuits and expanded their horizons in every sense. One eager driver was able to coax his vehicle to 120 miles an hour — in Brooklyn! ', 'Fortunately, no crackups resulted. ', 'Soon cars were changing everything. New suburban rings bloomed around cities, joining the urban and the rural to create something that was not quite either. An accidental effect of the war was a significant boost in oil production, as a result of a heavy emphasis on machines like tanks and airplanes that used internal combustion engines. Americans were quick to corral these markets, just as they embraced the automobile like no one else. But prewar experimentation in steam-powered and electric vehicles waned as a result. ', 'To better support this all-important vehicle, thousands of miles of highway were built, and that too changed the country forever. To promote the idea of interstate travel, a military convoy left Washington for California in July 1919. The New York Times called it “the largest aggregation of motor vehicles ever started on a trip of such length.”', 'But the convoy broke down repeatedly, and took 62 days to reach its destination. It averaged just six miles an hour, and almost didn’t make it out of Utah. As it turned out, there were almost no paved roads between Illinois and Nevada. Decades later, the officer who led the convoy, Dwight D. Eisenhower, would push for a national highway system as president. Even with a well-publicized divide between red and blue states, we can generally reach each other when we need to, and that is another unexpected result of a pivotal year.', 'In his overquoted poem from 1919, Yeats wrote “things fall apart, the center cannot hold.” Those words often ring true today, in another divided moment. But one of the comforts of history is seeing how wrong the earlier prognosticators of doom could be. By studying the ways in which we failed to fall apart, perhaps we can glean a small harvest of hope for the year ahead.', '\""
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(corpus_nltk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = (corpus_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowball = SnowballStemmer('english')\n",
    "# snowball_tokenized = [snowball.stem(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def snowball_tokenize(doc):\n",
    "    snowball = SnowballStemmer('english')\n",
    "    return [snowball.stem(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = snowball_tokenize(corpus_nltk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " ',',\n",
       " 'in',\n",
       " 'his',\n",
       " 'essay',\n",
       " '“',\n",
       " 'the',\n",
       " 'crack-up',\n",
       " ',',\n",
       " '”',\n",
       " 'f.',\n",
       " 'scott',\n",
       " 'fitzgerald',\n",
       " 'wrote',\n",
       " ',',\n",
       " '“',\n",
       " 'the',\n",
       " 'test',\n",
       " 'of',\n",
       " 'a',\n",
       " 'first-rat',\n",
       " 'intellig',\n",
       " 'is',\n",
       " 'the',\n",
       " 'abil',\n",
       " 'to',\n",
       " 'hold',\n",
       " 'two',\n",
       " 'oppos',\n",
       " 'idea',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mind',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " ',',\n",
       " 'and',\n",
       " 'still',\n",
       " 'retain',\n",
       " 'the',\n",
       " 'abil',\n",
       " 'to',\n",
       " 'function.',\n",
       " '”',\n",
       " \"'\",\n",
       " ',',\n",
       " 'on',\n",
       " 'new',\n",
       " 'year',\n",
       " '’',\n",
       " 's',\n",
       " 'day',\n",
       " '1919',\n",
       " ',',\n",
       " 'the',\n",
       " 'headlin',\n",
       " 'in',\n",
       " 'the',\n",
       " 'new',\n",
       " 'york',\n",
       " 'time',\n",
       " 'gave',\n",
       " 'a',\n",
       " 'hint',\n",
       " 'of',\n",
       " 'how',\n",
       " 'difficult',\n",
       " 'that',\n",
       " 'would',\n",
       " 'be',\n",
       " 'for',\n",
       " 'american',\n",
       " ',',\n",
       " 'struggl',\n",
       " 'to',\n",
       " 'live',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'shimmer',\n",
       " 'promis',\n",
       " 'they',\n",
       " 'had',\n",
       " 'made',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'dure',\n",
       " 'the',\n",
       " 'great',\n",
       " 'war',\n",
       " ',',\n",
       " 'which',\n",
       " 'had',\n",
       " 'end',\n",
       " 'just',\n",
       " 'over',\n",
       " 'a',\n",
       " 'month',\n",
       " 'befor',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'pole',\n",
       " ',',\n",
       " 'newli',\n",
       " 'independ',\n",
       " 'but',\n",
       " 'alreadi',\n",
       " 'threaten',\n",
       " 'by',\n",
       " 'their',\n",
       " 'neighbor',\n",
       " ',',\n",
       " 'were',\n",
       " 'call',\n",
       " 'on',\n",
       " 'american',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'them',\n",
       " '.',\n",
       " 'other',\n",
       " 'countri',\n",
       " ',',\n",
       " 'like',\n",
       " 'ireland',\n",
       " ',',\n",
       " 'were',\n",
       " 'on',\n",
       " 'the',\n",
       " 'cusp',\n",
       " 'of',\n",
       " 'independ',\n",
       " ',',\n",
       " 'but',\n",
       " 'ask',\n",
       " 'for',\n",
       " 'help',\n",
       " '.',\n",
       " 'a',\n",
       " 'bewild',\n",
       " 'set',\n",
       " 'of',\n",
       " 'new',\n",
       " 'respons',\n",
       " 'was',\n",
       " 'settl',\n",
       " 'upon',\n",
       " 'a',\n",
       " 'countri',\n",
       " 'that',\n",
       " 'had',\n",
       " 'long',\n",
       " 'cherish',\n",
       " 'it',\n",
       " 'freedom',\n",
       " 'from',\n",
       " 'the',\n",
       " 'old',\n",
       " 'world',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'overwhelm',\n",
       " ',',\n",
       " 'american',\n",
       " 'long',\n",
       " 'to',\n",
       " 'get',\n",
       " 'their',\n",
       " 'old',\n",
       " 'live',\n",
       " 'back',\n",
       " '.',\n",
       " 'but',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'clear',\n",
       " 'road',\n",
       " 'map',\n",
       " 'to',\n",
       " '“',\n",
       " 'normalci',\n",
       " ',',\n",
       " '”',\n",
       " 'to',\n",
       " 'use',\n",
       " 'a',\n",
       " 'word',\n",
       " 'that',\n",
       " 'began',\n",
       " 'to',\n",
       " 'gain',\n",
       " 'traction',\n",
       " 'in',\n",
       " 'the',\n",
       " 'chao',\n",
       " 'of',\n",
       " 'the',\n",
       " 'postwar',\n",
       " 'year',\n",
       " '.',\n",
       " 'grammarian',\n",
       " 'fault',\n",
       " 'it',\n",
       " 'as',\n",
       " 'a',\n",
       " 'clumsi',\n",
       " 'neolog',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'onli',\n",
       " 'made',\n",
       " 'it',\n",
       " 'more',\n",
       " 'popular',\n",
       " '.',\n",
       " 'a',\n",
       " 'rakish',\n",
       " 'ohio',\n",
       " 'senat',\n",
       " ',',\n",
       " 'warren',\n",
       " 'hard',\n",
       " ',',\n",
       " 'would',\n",
       " 'eventu',\n",
       " 'ride',\n",
       " 'it',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'the',\n",
       " 'white',\n",
       " 'hous',\n",
       " '.',\n",
       " 'in',\n",
       " 'a',\n",
       " 'crack',\n",
       " 'old',\n",
       " 'record',\n",
       " ',',\n",
       " 'he',\n",
       " 'can',\n",
       " 'be',\n",
       " 'heard',\n",
       " 'promis',\n",
       " ',',\n",
       " 'vagu',\n",
       " ',',\n",
       " 'to',\n",
       " 'make',\n",
       " 'america',\n",
       " 'great',\n",
       " 'again',\n",
       " '.',\n",
       " 'it',\n",
       " 'didn',\n",
       " '’',\n",
       " 't',\n",
       " 'work',\n",
       " 'out',\n",
       " 'well',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'there',\n",
       " 'were',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'believ',\n",
       " 'lofti',\n",
       " 'promis',\n",
       " 'as',\n",
       " '1919',\n",
       " 'was',\n",
       " 'begin',\n",
       " '.',\n",
       " 'with',\n",
       " 'peac',\n",
       " ',',\n",
       " 'everyth',\n",
       " 'felt',\n",
       " 'new',\n",
       " 'again',\n",
       " '.',\n",
       " 'that',\n",
       " 'was',\n",
       " 'certain',\n",
       " 'true',\n",
       " 'in',\n",
       " 'the',\n",
       " 'geopolit',\n",
       " 'realm',\n",
       " ',',\n",
       " 'as',\n",
       " 'negoti',\n",
       " 'conven',\n",
       " 'in',\n",
       " 'pari',\n",
       " ',',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'redraw',\n",
       " 'the',\n",
       " 'world',\n",
       " 'map',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'know',\n",
       " 'where',\n",
       " 'to',\n",
       " 'begin',\n",
       " '.',\n",
       " 'the',\n",
       " 'german',\n",
       " ',',\n",
       " 'austrian',\n",
       " 'and',\n",
       " 'ottoman',\n",
       " 'empir',\n",
       " 'were',\n",
       " 'now',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ash',\n",
       " 'heap',\n",
       " 'of',\n",
       " 'histori',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'memor',\n",
       " 'phrase',\n",
       " 'of',\n",
       " 'leon',\n",
       " 'trotski',\n",
       " '.',\n",
       " 'his',\n",
       " 'countri',\n",
       " ',',\n",
       " 'russia',\n",
       " ',',\n",
       " 'had',\n",
       " 'not',\n",
       " 'disappear',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'undergo',\n",
       " 'a',\n",
       " 'transform',\n",
       " 'so',\n",
       " 'profound',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'destabil',\n",
       " 'half',\n",
       " 'of',\n",
       " 'europ',\n",
       " '.',\n",
       " 'oversea',\n",
       " ',',\n",
       " 'territori',\n",
       " 'belong',\n",
       " 'to',\n",
       " 'the',\n",
       " 'belliger',\n",
       " 'were',\n",
       " 'be',\n",
       " 'exchang',\n",
       " 'like',\n",
       " 'carpet',\n",
       " 'at',\n",
       " 'a',\n",
       " 'second-hand',\n",
       " 'bazaar',\n",
       " '.',\n",
       " 'a',\n",
       " 'centuri',\n",
       " 'later',\n",
       " ',',\n",
       " 'from',\n",
       " 'the',\n",
       " 'middl',\n",
       " 'east',\n",
       " 'to',\n",
       " 'africa',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " 'is',\n",
       " 'still',\n",
       " 'struggl',\n",
       " 'with',\n",
       " 'the',\n",
       " 'decis',\n",
       " 'made',\n",
       " 'hastili',\n",
       " 'in',\n",
       " '1919',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'but',\n",
       " 'if',\n",
       " 'anyon',\n",
       " 'could',\n",
       " 'solv',\n",
       " 'these',\n",
       " 'nasti',\n",
       " 'problem',\n",
       " ',',\n",
       " 'american',\n",
       " 'could',\n",
       " '.',\n",
       " 'to',\n",
       " 'a',\n",
       " 'beleagu',\n",
       " 'world',\n",
       " ',',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'almost',\n",
       " 'superhuman',\n",
       " '.',\n",
       " 'despit',\n",
       " 'their',\n",
       " 'late',\n",
       " 'entri',\n",
       " 'into',\n",
       " 'the',\n",
       " 'war',\n",
       " ',',\n",
       " 'they',\n",
       " 'had',\n",
       " 'shape',\n",
       " 'the',\n",
       " 'conflict',\n",
       " 'decis',\n",
       " 'with',\n",
       " 'their',\n",
       " 'manpow',\n",
       " ',',\n",
       " 'their',\n",
       " 'matériel',\n",
       " 'and',\n",
       " 'their',\n",
       " 'chirpi',\n",
       " 'market',\n",
       " '.',\n",
       " '“',\n",
       " 'democraci',\n",
       " '”',\n",
       " 'was',\n",
       " 'the',\n",
       " 'word',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hour',\n",
       " ',',\n",
       " 'an',\n",
       " 'elixir',\n",
       " 'for',\n",
       " 'all',\n",
       " 'problem',\n",
       " ',',\n",
       " 'accord',\n",
       " 'to',\n",
       " 'presid',\n",
       " 'woodrow',\n",
       " 'wilson',\n",
       " ',',\n",
       " 'who',\n",
       " 'inton',\n",
       " 'it',\n",
       " 'with',\n",
       " 'impress',\n",
       " 'solemn',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'for',\n",
       " 'a',\n",
       " 'time',\n",
       " ',',\n",
       " 'the',\n",
       " 'world',\n",
       " 'believ',\n",
       " 'it',\n",
       " '.',\n",
       " 'as',\n",
       " 'he',\n",
       " 'head',\n",
       " 'to',\n",
       " 'pari',\n",
       " ',',\n",
       " 'for',\n",
       " 'what',\n",
       " 'was',\n",
       " 'the',\n",
       " 'longest',\n",
       " 'oversea',\n",
       " 'trip',\n",
       " 'of',\n",
       " 'a',\n",
       " 'presid',\n",
       " ',',\n",
       " 'he',\n",
       " 'was',\n",
       " 'lioniz',\n",
       " 'as',\n",
       " 'few',\n",
       " 'leader',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'been',\n",
       " '.',\n",
       " 'film',\n",
       " 'footag',\n",
       " 'captur',\n",
       " 'the',\n",
       " 'odd',\n",
       " 'contrast',\n",
       " 'of',\n",
       " 'his',\n",
       " 'quiet',\n",
       " 'ocean',\n",
       " 'passag',\n",
       " ',',\n",
       " 'follow',\n",
       " 'by',\n",
       " 'the',\n",
       " 'euphoria',\n",
       " 'of',\n",
       " 'the',\n",
       " 'crowd',\n",
       " 'in',\n",
       " 'franc',\n",
       " ',',\n",
       " 'who',\n",
       " 'hope',\n",
       " 'he',\n",
       " 'would',\n",
       " 'solv',\n",
       " 'all',\n",
       " 'of',\n",
       " 'their',\n",
       " 'problem',\n",
       " '.',\n",
       " 'that',\n",
       " 'was',\n",
       " 'a',\n",
       " 'tall',\n",
       " 'order',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'we',\n",
       " 'still',\n",
       " 'live',\n",
       " 'close',\n",
       " 'to',\n",
       " 'that',\n",
       " 'graini',\n",
       " 'footag',\n",
       " ',',\n",
       " 'insid',\n",
       " 'the',\n",
       " 'world',\n",
       " 'that',\n",
       " 'emerg',\n",
       " 'from',\n",
       " 'versaill',\n",
       " 'and',\n",
       " 'it',\n",
       " 'hall',\n",
       " 'of',\n",
       " 'mirror',\n",
       " '.',\n",
       " 'german',\n",
       " 'resent',\n",
       " 'of',\n",
       " 'the',\n",
       " 'harsh',\n",
       " 'condit',\n",
       " 'impos',\n",
       " 'fuel',\n",
       " 'the',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'adolf',\n",
       " 'hitler',\n",
       " ',',\n",
       " 'who',\n",
       " ',',\n",
       " 'like',\n",
       " 'mani',\n",
       " ',',\n",
       " 'found',\n",
       " 'his',\n",
       " 'polit',\n",
       " 'voic',\n",
       " 'in',\n",
       " '1919.',\n",
       " 'historian',\n",
       " 'would',\n",
       " 'later',\n",
       " 'trace',\n",
       " 'the',\n",
       " 'origin',\n",
       " 'of',\n",
       " 'world',\n",
       " 'war',\n",
       " 'ii',\n",
       " 'to',\n",
       " 'the',\n",
       " 'treati',\n",
       " '’',\n",
       " 's',\n",
       " 'shortcom',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'swirl',\n",
       " 'nation',\n",
       " 'that',\n",
       " 'it',\n",
       " 'fail',\n",
       " 'to',\n",
       " 'calm',\n",
       " '.',\n",
       " 'presid',\n",
       " 'wilson',\n",
       " 'promis',\n",
       " 'to',\n",
       " 'build',\n",
       " 'a',\n",
       " 'world',\n",
       " '“',\n",
       " 'safe',\n",
       " 'for',\n",
       " 'democraci',\n",
       " ',',\n",
       " '”',\n",
       " 'but',\n",
       " 'the',\n",
       " 'result',\n",
       " 'was',\n",
       " 'neither',\n",
       " 'safe',\n",
       " 'nor',\n",
       " ',',\n",
       " 'in',\n",
       " 'mani',\n",
       " 'case',\n",
       " ',',\n",
       " 'democrat',\n",
       " '.',\n",
       " 'a',\n",
       " 'british',\n",
       " 'politician',\n",
       " 'whose',\n",
       " 'career',\n",
       " 'bare',\n",
       " 'surviv',\n",
       " 'the',\n",
       " 'war',\n",
       " ',',\n",
       " 'winston',\n",
       " 'churchil',\n",
       " ',',\n",
       " 'saw',\n",
       " 'it',\n",
       " 'more',\n",
       " 'clear',\n",
       " 'as',\n",
       " 'a',\n",
       " '“',\n",
       " 'crippl',\n",
       " ',',\n",
       " 'broken',\n",
       " 'world',\n",
       " ',',\n",
       " '”',\n",
       " 'divid',\n",
       " 'along',\n",
       " 'fault',\n",
       " 'line',\n",
       " 'that',\n",
       " 'had',\n",
       " 'never',\n",
       " 'been',\n",
       " 'obliter',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'larg',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'british',\n",
       " ',',\n",
       " 'french',\n",
       " 'and',\n",
       " 'belgian',\n",
       " 'empir',\n",
       " 'remain',\n",
       " 'intact',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'distress',\n",
       " 'of',\n",
       " 'million',\n",
       " 'hope',\n",
       " 'for',\n",
       " 'democraci',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " '.',\n",
       " 'inspir',\n",
       " 'by',\n",
       " 'the',\n",
       " 'wind',\n",
       " 'of',\n",
       " 'chang',\n",
       " ',',\n",
       " 'independ',\n",
       " 'movement',\n",
       " 'sprang',\n",
       " 'up',\n",
       " 'around',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'some',\n",
       " 'were',\n",
       " 'success',\n",
       " ',',\n",
       " 'but',\n",
       " 'for',\n",
       " 'mani',\n",
       " 'other',\n",
       " ',',\n",
       " 'self-determin',\n",
       " ',',\n",
       " 'anoth',\n",
       " 'one',\n",
       " 'of',\n",
       " 'wilson',\n",
       " '’',\n",
       " 's',\n",
       " 'ring',\n",
       " 'phrase',\n",
       " ',',\n",
       " 'was',\n",
       " 'elus',\n",
       " 'at',\n",
       " 'best',\n",
       " '.',\n",
       " 'weari',\n",
       " 'european',\n",
       " 'diplomat',\n",
       " 'chafe',\n",
       " 'at',\n",
       " 'his',\n",
       " 'moralist',\n",
       " 'speech',\n",
       " ',',\n",
       " 'which',\n",
       " 'they',\n",
       " 'scorn',\n",
       " 'as',\n",
       " '“',\n",
       " 'sermonettes.',\n",
       " '”',\n",
       " 'the',\n",
       " 'presid',\n",
       " 'of',\n",
       " 'franc',\n",
       " ',',\n",
       " 'georg',\n",
       " 'clemenceau',\n",
       " ',',\n",
       " 'complain',\n",
       " 'that',\n",
       " '“',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'wilson',\n",
       " 'was',\n",
       " 'someth',\n",
       " 'like',\n",
       " 'convers',\n",
       " 'with',\n",
       " 'jesus',\n",
       " 'christ',\n",
       " '!',\n",
       " '”',\n",
       " 'wilson',\n",
       " 'would',\n",
       " 'sure',\n",
       " 'have',\n",
       " 'second',\n",
       " 'the',\n",
       " 'thought',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'still',\n",
       " ',',\n",
       " 'there',\n",
       " 'were',\n",
       " 'pocket',\n",
       " 'of',\n",
       " 'realism',\n",
       " 'within',\n",
       " 'the',\n",
       " 'ideal',\n",
       " 'of',\n",
       " 'a',\n",
       " 'politician',\n",
       " 'who',\n",
       " ',',\n",
       " 'for',\n",
       " 'all',\n",
       " 'his',\n",
       " 'blind',\n",
       " 'spot',\n",
       " ',',\n",
       " 'understood',\n",
       " 'that',\n",
       " 'the',\n",
       " 'world',\n",
       " 'could',\n",
       " 'not',\n",
       " 'continu',\n",
       " 'as',\n",
       " 'befor',\n",
       " '.',\n",
       " 'wilson',\n",
       " 'has',\n",
       " 'long',\n",
       " 'been',\n",
       " 'critic',\n",
       " 'for',\n",
       " 'his',\n",
       " 'failur',\n",
       " ',',\n",
       " 'but',\n",
       " 'he',\n",
       " 'was',\n",
       " 'correct',\n",
       " 'in',\n",
       " 'his',\n",
       " 'grasp',\n",
       " 'of',\n",
       " 'a',\n",
       " 'core',\n",
       " 'truth',\n",
       " ',',\n",
       " 'still',\n",
       " 'relev',\n",
       " ',',\n",
       " 'that',\n",
       " 'a',\n",
       " 'lazi',\n",
       " 'return',\n",
       " 'to',\n",
       " 'unbridl',\n",
       " 'nation',\n",
       " 'was',\n",
       " 'a',\n",
       " 'recip',\n",
       " 'for',\n",
       " 'conflict',\n",
       " '.',\n",
       " 'like',\n",
       " 'most',\n",
       " 'presid',\n",
       " ',',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'relish',\n",
       " 'consult',\n",
       " 'with',\n",
       " 'congress',\n",
       " ',',\n",
       " 'a',\n",
       " 'misread',\n",
       " 'of',\n",
       " 'democraci',\n",
       " 'that',\n",
       " 'boomerang',\n",
       " 'on',\n",
       " 'him',\n",
       " ',',\n",
       " 'as',\n",
       " 'a',\n",
       " 'newli',\n",
       " 'elect',\n",
       " 'republican',\n",
       " 'major',\n",
       " 'refus',\n",
       " 'to',\n",
       " 'approv',\n",
       " 'his',\n",
       " 'plan',\n",
       " 'for',\n",
       " 'a',\n",
       " 'leagu',\n",
       " 'of',\n",
       " 'nation',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " 'desper',\n",
       " 'to',\n",
       " 'outflank',\n",
       " 'them',\n",
       " ',',\n",
       " 'he',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'take',\n",
       " 'his',\n",
       " 'case',\n",
       " 'direct',\n",
       " 'to',\n",
       " 'the',\n",
       " 'peopl',\n",
       " ',',\n",
       " 'with',\n",
       " 'boister',\n",
       " 'ralli',\n",
       " ',',\n",
       " 'far',\n",
       " 'from',\n",
       " 'east',\n",
       " 'coast',\n",
       " 'elit',\n",
       " '—',\n",
       " 'anoth',\n",
       " 'way',\n",
       " 'in',\n",
       " 'which',\n",
       " '2019',\n",
       " 'mirror',\n",
       " '1919.',\n",
       " 'the',\n",
       " 'strain',\n",
       " 'of',\n",
       " 'that',\n",
       " 'effort',\n",
       " 'result',\n",
       " 'in',\n",
       " 'a',\n",
       " 'stroke',\n",
       " ',',\n",
       " 'which',\n",
       " 'near',\n",
       " 'incapacit',\n",
       " 'wilson',\n",
       " 'for',\n",
       " 'the',\n",
       " 'last',\n",
       " 'year',\n",
       " 'of',\n",
       " 'his',\n",
       " 'presid',\n",
       " '.',\n",
       " 'democraci',\n",
       " '’',\n",
       " 's',\n",
       " 'great',\n",
       " 'spokesman',\n",
       " 'had',\n",
       " 'imperil',\n",
       " 'the',\n",
       " 'veri',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " 'that',\n",
       " 'he',\n",
       " ...]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list = list(pd.ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 ms, sys: 1.21 ms, total: 17 ms\n",
      "Wall time: 17.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create vectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(#input='content', \n",
    "#                 encoding='utf-8', \n",
    "#                 decode_error='strict', \n",
    "                 strip_accents=None, \n",
    "                 lowercase=True, \n",
    "#                 preprocessor=None, \n",
    "                 tokenizer=snowball_tokenize, \n",
    "#                 analyzer='word', \n",
    "                 stop_words='english', \n",
    "#                 token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "#                 ngram_range=(1, 1), \n",
    "#                 max_df=1.0, \n",
    "#                 min_df=1, \n",
    "                 max_features=None, \n",
    "#                 vocabulary=None, \n",
    "#                 binary=False, \n",
    "#                 dtype=<class 'numpy.float64'>, \n",
    "#                 norm='l2', \n",
    "#                 use_idf=True, \n",
    "#                 smooth_idf=True, \n",
    "#                 sublinear_tf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 6s, sys: 6.95 s, total: 11min 13s\n",
      "Wall time: 11min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_snowball = vectorizer.fit_transform(corpus_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class sklearn.feature_extraction.text.CountVectorizer(*, \n",
    "#                                                       input='content', \n",
    "#                                                       encoding='utf-8', \n",
    "#                                                       decode_error='strict', \n",
    "#                                                       strip_accents=None, \n",
    "#                                                       lowercase=True, \n",
    "#                                                       preprocessor=None, \n",
    "#                                                       tokenizer=None, \n",
    "#                                                       stop_words=None, \n",
    "#                                                       token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "#                                                       ngram_range=(1, 1), \n",
    "#                                                       analyzer='word', \n",
    "#                                                       max_df=1.0, \n",
    "#                                                       min_df=1, \n",
    "#                                                       max_features=None, \n",
    "#                                                       vocabulary=None, \n",
    "#                                                       binary=False, \n",
    "#                                                       dtype=<class 'numpy.int64'>\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(strip_accents='None',\n",
    "                                   lowercase=True,\n",
    "                                   tokenizer=snowball_tokenize,\n",
    "                                   stop_words='english',\n",
    "                                   max_features=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #balance the classes\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #X, y --> X_resampled, y_resampled\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_snowball, y_nltk)\n",
    "# print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #test, train, split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# rf_clf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #clf.score = accuracy = 'true'(pos/neg) / total\n",
    "\n",
    "# rf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = y_pred).ravel()\n",
    "# (tn, fp, fn, tp)\n",
    "# metrics_(tn, fp, fn, tp)\n",
    "# print(f'tn={tn}, fp={fp}, fn={fn}, tp={tp})')\n",
    "# print(rf_clf.n_features_)\n",
    "# print(rf_clf.n_classes_)\n",
    "# print(rf_clf.n_outputs_)\n",
    "# # what are the actual parametrs set in the function. something specific about the random forest i forgot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop = vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37728, 254169)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_snowball.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_import = rf_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property feature_importances_\n",
    "# The impurity-based feature importances.\n",
    "\n",
    "# The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n",
    "\n",
    "# Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See sklearn.inspection.permutation_importance as an alternative.\n",
    "\n",
    "# Returns\n",
    "# feature_importances_ndarray of shape (n_features,)\n",
    "# The values of this array sum to 1, unless all trees are single node trees consisting of only the root node, in which case it will be an array of zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_scores = pd.Series(feature_import,\n",
    "#                            index=nltk_features)\n",
    "# feat_scores = feat_scores.sort_values()\n",
    "# ax = feat_scores.plot(kind='barh', figsize=(10,20))\n",
    "# ax.set_title('\"Decrease in Impurity\" Importance')\n",
    "# ax.set_xlabel('Average contribution to the reduction in variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = permutation_importance(rf_clf, X_test, y_test, n_repeats=30, random_state=0)\n",
    "# for index in r.importances_mean.argsort()[::-1]:\n",
    "#     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#         print(f\"{feature_names[i]:<8}\"\n",
    "#               f\"{r.importances_mean[i]:.3f}\"\n",
    "#               f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# test_x = test_vectorizer.fit_transform(corpus_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3957), (1, 3957)]\n",
      "CPU times: user 26.6 ms, sys: 12.8 ms, total: 39.4 ms\n",
      "Wall time: 39.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#balance the classes\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "#X, y --> X_resampled, y_resampled\n",
    "X_resampled, y_resampled = rus.fit_resample(X_snowball, y_nltk)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 ms, sys: 7.53 ms, total: 17.4 ms\n",
      "Wall time: 16.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test, train, split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 673 ms, sys: 28.2 ms, total: 702 ms\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8388074785245073\n",
      "recall = 0.7663366336633664\n",
      "precision = 0.9031505250875146\n",
      "tn=886, fp=83, fn=236, tp=774)\n",
      "254169\n",
      "2\n",
      "1\n",
      "254169\n",
      "<class 'numpy.ndarray'>\n",
      "CPU times: user 424 ms, sys: 125 ms, total: 549 ms\n",
      "Wall time: 549 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(254169,)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = y_pred).ravel()\n",
    "(tn, fp, fn, tp)\n",
    "metrics_(tn, fp, fn, tp)\n",
    "print(f'tn={tn}, fp={fp}, fn={fn}, tp={tp})')\n",
    "print(rf_clf.n_features_)\n",
    "print(rf_clf.n_classes_)\n",
    "print(rf_clf.n_outputs_)\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "print(len(feat_names))\n",
    "feature_import = rf_clf.feature_importances_\n",
    "print(type(feature_import))\n",
    "feature_import.shape\n",
    "# what are the actual parametrs set in the function. something specific about the random forest i forgot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1979, 254169)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1979,)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.datetime.now())\n",
    "feat_scores = pd.Series(feature_import[:20],\n",
    "                           index=feat_names[:20])\n",
    "feat_scores = feat_scores.sort_values()\n",
    "ax = feat_scores.plot(kind='barh', figsize=(10,20))\n",
    "ax.set_title('\"Decrease in Impurity\" Importance')\n",
    "ax.set_xlabel('Average contribution to the reduction in variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.datetime.now())g\n",
    "r = permutation_importance(rf_clf, X_test.toarray(), y_test, n_repeats=30, random_state=0)\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(f\"{feat_names[i]:<8}\"\n",
    "              f\"{r.importances_mean[i]:.3f}\"\n",
    "              f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
