{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline \n",
    "\n",
    "![](images/pipeline-walkthrough1.png)\n",
    "\n",
    "Below is a to do list when converting text into vector form: \n",
    "\n",
    "**Clean text and Create a Bag of Words (BoW)**\n",
    ">1. Lowercase the text\n",
    "2. Tokenize \n",
    "3. Strip out punctuation or undesirable text\n",
    "4. Remove Stopwords \n",
    "5. Stemming or Lemmatizing\n",
    "6. Compute N-Grams\n",
    "7. Use this to create BoW\n",
    "\n",
    "**Vectorize BoW**\n",
    ">8. Term Frequencies\n",
    "9. Document Frequencies\n",
    "10. TF-IDF\n",
    "11. Normalize vectors\n",
    "\n",
    "Let's go through both what each of these steps are and how to do them in python with the following corpus of comments about data science...\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to vectorize the type_of_material series into a y target vector.\n",
    "\n",
    "def vectorize_y_ser(ser):\n",
    "    y = ser.copy()\n",
    "    y.replace({'Op-Ed': 1,'News': 0}, inplace=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate of correct predictions out of total predictions\n",
    "\n",
    "def metrics_(tn, fp, fn, tp):\n",
    "    accuracy = (tp + tn) / (tn + fn + tp + fp)\n",
    "    print(f'accuracy = {accuracy}')\n",
    "    recall = (tp) / (tp + fn)\n",
    "    print(f'recall = {recall}')\n",
    "    precision = (tp) / (tp + fp)\n",
    "    print(f'precision = {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that removes string.punctuation w/out the '?'\n",
    "def punc_strip(string):\n",
    "    my_lil_punc_string = '!\"#$$%&\\()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "    extras = '[\\',.â€œ-â€â€™!â€”:()?@]$;\"â€“/#|&â€˜Â°\\\\â€¦%<â€+Â¡Â«Â»_â€¢*>â€º=â‚¬Â·â– â™¦Â£ğŸ’ƒğŸ½âˆ™â‰ {}Â¿ğŸ˜‚Â¥Ì‡Ìƒâ˜…Â§ğŸ‡ºğŸ‡¸Â©ï¼ğŸ’£ã€‚^â€‘ğŸ”¥ğŸ˜¡â€³Ì€ğŸš¨âƒ£ğŸ˜¤ğŸŒ«`â—ğŸ¤­âœ“ğŸ“–ğŸ“¹âœ‹ğŸ‘€Â¢ğŸ’”ğŸ˜â€’ğŸ“²ğŸ“ºğŸ™ï¿¼ğŸ˜â†“â¤ï¸ğŸ˜˜ğŸ‰ğŸ¥‚ğŸ¦”ğŸ¥©ğŸğŸ‘ğŸ¤¸ğŸ•³ğŸŒµâ›½âš•ğŸ”Šâ„¢â€ â€¾â€²Ìˆ×´Ã·ğŸ’œğŸ’›Â¨Â´ğŸ’¥â˜„Ì~â€šâŒšâ†‘ğŸğŸ‡°ğŸ‡ªğŸŒâ†’ğŸ—£â”€â€¼â—‡ğŸ˜¾ğŸ¤·ğŸ»â™‚âœ”ğŸ’ªÂ®â™ªğŸ˜±ğŸ‘ØŒÂ¯âŒ˜ğŸŒˆğŸ¤©âœ¨ÌŠğŸ¤£âˆ’ğŸ˜‰â™¥ğŸ‘ğŸ’©ğŸ¤ğŸ’‹ğŸ§“ğŸ‘®ğŸ‘´â™€ğŸ¥´ğŸâš¡ğŸ¤”ğŸ™Ã—ğŸ’¯ğŸš‚ğŸ™ğŸ¾à¹ˆà¦¾à¦‚à¦¼à§‡à§à¦¿à§ğŸ†ğŸ’¦ğŸ¥ğŸ’Œâ—¾ğŸ“±ğŸ‘‹ğŸ¼ØŸğŸ™‚ğŸš«â—ğŸ™„ğŸ›‘ğŸŒ§â„Ì¶ğŸ¤ï¼›ğŸ¤¯ğŸ§µğŸ˜¹Â¶â¡ğŸ˜ğŸ”§âœŒâ¼â—‰ğŸ±âšªÌ§Ì„â”ğŸ˜ºğŸ˜ğŸ‡¬ğŸ‡§ğŸ‘ğŸº'\n",
    "    punc_list = my_lil_punc_string + extras\n",
    "    \n",
    "    for char in string:\n",
    "        if char in punc_list:  \n",
    "            string = string.replace(char, \"\")\n",
    "            \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 276 ms, total: 1.55 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import csv\n",
    "\n",
    "_2019_data = pd.read_csv('data/capstone2_2019_final.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 619 Âµs, sys: 50 Âµs, total: 669 Âµs\n",
      "Wall time: 659 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_2019_df = _2019_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30256, 2)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2019_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                ', 'In 2019, hereâ€™s what we could do instead.'...\n",
       "type_of_material                                                Op-Ed\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_2019_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nan = _2019_df.text.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "30251     True\n",
       "30252    False\n",
       "30253    False\n",
       "30254    False\n",
       "30255    False\n",
       "Name: text, Length: 30256, dtype: bool"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30256"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                ', 'In 2019, hereâ€™s what we could do instead.'...\n",
       "type_of_material                                                Op-Ed\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_2019_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30256, 2)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2019_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_nan_2019_df = _2019_df[text_nan == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28484, 2)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_nan_2019_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(drop_nan_2019_df.type_of_material == \"News\")\n",
    "\n",
    "# 1741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(drop_nan_2019_df.type_of_material == \"Op-Ed\")\n",
    "\n",
    "# 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                ', 'In 2019, hereâ€™s what we could do instead.'...\n",
       "type_of_material                                                Op-Ed\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(drop_nan_2019_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28213"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(_2019_df.type_of_material == \"News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(_2019_df.type_of_material == \"Op-Ed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2019_df = drop_nan_2019_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X. X is currently pandas series of unsplit strings\n",
    "\n",
    "X = _2019_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn series into list...\n",
    "\n",
    "corpus = list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define y as a series of op-ed or news\n",
    "\n",
    "y = _2019_df.type_of_material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.39 ms, sys: 722 Âµs, total: 9.11 ms\n",
      "Wall time: 8.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vectorize y in to (1, 0) (op-ed, news)\n",
    "\n",
    "y = vectorize_y_ser(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\', \\'In 2019, hereâ€™s what we could do instead.\\', \"In most Western states, that $1,380 you spent on your phone could buy half an acre of land. In the right conditions, that half acre could easily accommodate 150 trees. A single tree sequesters 48 pounds of carbon a year. It takes about 30 minutes for an amateur forester to plant a tree. If every American smartphone owner used that time and money to plant half an acre of trees, we would sequester about 886 million tons of carbon a year, enough to offset more than 10 percent of the countryâ€™s annual emissions. If you don\\'t want to do the planting yourself, the National Forest Foundation says it could meet all of its planting goals if every smartphone user gave it just 60 cents.\", \\'A recent study of romantic relationships among college students in the journal Psychology of Popular Media Culture found that â€œsmartphone dependency is significantly linked to relationship uncertaintyâ€ and that â€œpartnersâ€™ perceived smartphone dependency predicts less relationship satisfaction.â€ According to another recent study, more than 29 percent of Americans would rather give up sex for three months than give up their smartphone for a single week.\\', \\'Now flip that around: If you gave up your device for a year, you would have time to make love about 16,000 times (assuming youâ€™re like most Americans and your lovemaking sessions last an average of 5.4 minutes, not counting foreplay).\\', \\'If all that sex doesnâ€™t bring you and your partner closer, you could pay for about four hours of couples therapy. Not enough time? The renowned couples therapist Esther Perel has managed to fix some couplesâ€™ problems in three.\\', \\'Currently the American political system undercounts the votes of the majority of Americans, either through gerrymandering or the unfair distribution of Senate seats and electoral votes. But this system can be changed particularly if we push for a program for voter reform at a grassroots level. As David Gold, an attorney with the organization Democratism, noted, â€œQuitting devices would give citizens enough time and money to visit their local and state representatives three times a week for a year and cover the cost of the trip in gas or mass transit to lobby for reform.â€\\', \\'Every year 10 million tons of plastic waste flows into the ocean. According to George Leonard of the Ocean Conservancy, if Americans applied all the money they allocate to smartphones to solving plastic pollution, â€œThere would be enough money available to pay for the necessary improvements in waste management in Asian countries for 70 years.â€ And if the time Americans spent on smartphones were applied to ocean clean up at a rate of five pounds of plastic garbage per person per hour, â€œThe volunteer effort could clean up the amount of plastic that flows into the global ocean 118 times over.â€\\', \\'The average reader, reading at a speed of 280 words per minute, would take approximately 71Â½ hours to read the 1.3 million words in Marcel Proustâ€™s â€œIn Search of Lost Time.â€ With 1,460 hours repurposed from device usage, a reader would get through the books almost 20 times. With the $1,380 in device-free savings, you could spend the weekend in Illiers-Combray, the setting of Proustâ€™s first madeleine-soaked memories, and see if he got it right.\\', \\'According to the Mayo Clinic, swimming, walking or running for 30 minutes a day will lower your blood pressure by four to nine millimeters of mercury, as much or more than some blood pressure medication. Yes, you could keep your phone with you while you exercise, but who needs the stress? And if youâ€™d rather not exercise, blood pressure medication costs about $900 per year.\\', \\'The average American spends $14,000 per decade on smartphones. Thatâ€™s $70,000 over the course of an average working life. Invested in a conservative mutual fund with an annual rate of return of 4 percent, that would yield over $1.3 million in retirement savings. (The current median household retirement savings is $5,000.)\\', \\'Last year the globe-circling Scottish cyclist Mark Beaumont smashed the world circumnavigation record by riding around the worldâ€™s land mass in 79 days. He pedaled 16 hours a day for a total of 1,264 hours â€” or just under a yearâ€™s worth of smartphone usage. Average humans couldnâ€™t match Mr. Beaumontâ€™s feat, but the money and time saved by ditching their phones would afford them a lot of time with a personal trainer.\\', \\'Smartphone usage is highest among teenagers and people in their early 20s. And itâ€™s at this crucial time when virtuosity in a musical instrument can be attained. At current rates of device usage, most young people will burn through the famous 10,000 hours Malcolm Gladwell associated with becoming an â€œelite pianistâ€ over the course of the next decade. How many virtuosos will we lose in the years ahead if device use among young people continues to grow apace?\\', \\'Using English as a baseline, you would need approximately 700 hours to become proficient in a foreign language as measured by Common European Framework of Reference for Languages. With the time you spend staring into your device, you could learn two.\\', \\'A recent study found that children between 7 months and 24 months old experienced higher levels of distress and were less likely to investigate their surroundings when their parents were on their mobile devices. Secure attachment begins in infancy when children take visual cues of attachment from their parentsâ€™ gaze. Every moment you look at your infant instead of your phone is an investment in the future.\\', \\''"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type_of_material</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>', 'In 2019, hereâ€™s what we could do instead.'...</td>\n",
       "      <td>Op-Ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>', 'Many years before, back in Russia, the two...</td>\n",
       "      <td>Op-Ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>', 'At a critical moment in the film, just aft...</td>\n",
       "      <td>Op-Ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>', 'No other country in the world symbolizes t...</td>\n",
       "      <td>Op-Ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>', 'The cycle has to be reversed. In the Unite...</td>\n",
       "      <td>Op-Ed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text type_of_material\n",
       "0  ', 'In 2019, hereâ€™s what we could do instead.'...            Op-Ed\n",
       "1  ', 'Many years before, back in Russia, the two...            Op-Ed\n",
       "2  ', 'At a critical moment in the film, just aft...            Op-Ed\n",
       "3  ', 'No other country in the world symbolizes t...            Op-Ed\n",
       "4  ', 'The cycle has to be reversed. In the Unite...            Op-Ed"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2019_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming/lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "#wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 1 Âµs, total: 4 Âµs\n",
      "Wall time: 5.25 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def snowball_tokenize(doc):\n",
    "    snowball = SnowballStemmer('english')\n",
    "    return [snowball.stem(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = snowball_tokenize(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 Âµs, sys: 8 Âµs, total: 52 Âµs\n",
      "Wall time: 65.1 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create vectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(#input='content', \n",
    "#                 encoding='utf-8', \n",
    "#                 decode_error='strict', \n",
    "                 strip_accents='ascii', \n",
    "                 lowercase=True, \n",
    "#                 preprocessor=None, \n",
    "                 tokenizer=snowball_tokenize, \n",
    "#                 analyzer='word', \n",
    "                 stop_words=None, \n",
    "#                 token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "#                 ngram_range=(1, 1), \n",
    "                 max_df=0.85, \n",
    "                 min_df=0.15, \n",
    "                 max_features=None, \n",
    "#                 vocabulary=None, \n",
    "#                 binary=False, \n",
    "#                 dtype=<class 'numpy.float64'>, \n",
    "#                 norm='l2', \n",
    "#                 use_idf=True, \n",
    "#                 smooth_idf=True, \n",
    "#                 sublinear_tf=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-04 11:26:22.451030\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "X_snowball = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(strip_accents='None',\n",
    "                                   lowercase=True,\n",
    "                                   tokenizer=snowball_tokenize,\n",
    "                                   stop_words='english',\n",
    "                                   max_features=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#balance the classes\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "#X, y --> X_resampled, y_resampled\n",
    "X_resampled, y_resampled = rus.fit_resample(X_snowball, y_nltk)\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "print(rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test, train, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#test, train, split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'num_features = {rf_clf.n_features_}')\n",
    "feat_names = vectorizer.get_feature_names()\n",
    "feature_import = rf_clf.feature_importances_\n",
    "print(type(feature_import))\n",
    "feature_import.shape\n",
    "print(f'vectorizer = {vectorizer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.datetime.now())\n",
    "feat_scores = pd.Series(feature_import,\n",
    "                           index=feat_names)\n",
    "feat_scores = feat_scores.sort_values()\n",
    "ax = feat_scores[-20:].plot(kind='barh', figsize=(10,20))\n",
    "ax.set_title('\"Decrease in Impurity\" Importance')\n",
    "ax.set_xlabel('Average contribution to the reduction in variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(datetime.datetime.now())\n",
    "r = permutation_importance(rf_clf, X_test.toarray(), y_test, n_repeats=30, random_state=0)\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(f\"{feat_names[i]:<8}\"\n",
    "              f\"{r.importances_mean[i]:.3f}\"\n",
    "              f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_scores[-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
